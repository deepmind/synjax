{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "ozzWR2JKdGsA"
      },
      "outputs": [],
      "source": [
        "!pip install -qU matplotlib\n",
        "!pip install -qU git+https://github.com/deepmind/synjax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnxd2aT6DsIX"
      },
      "source": [
        "# Linear Chain CRF\n",
        "\n",
        "One common structure prediction problem is a task of labeling elements of a sequence. For instance, we may want to label a sequence of words with their part of speach tags (verb, noun, adjective etc.).\n",
        "\n",
        "One simple way of accomplishing that task is by having a contextual embedding of each word in the sentence and then independently labeling each word with a classifier. However, that does not account for correlations between a sequence of tags. For example, this simple model doesn't capture the fact that in English a sequence of labels \"adjective noun\" is much more likely than \"noun adjective\". To solve that we can use a type of model called Linear Chain Conditional Random Field (CRF) that explicitly models this relation amoung labels.\n",
        "\n",
        "CRF assigns a non-negative score, called potential, to each pair of labels at each point in the sequence. Concretelly, for an input sequence $\\boldsymbol{x} = [x_1, x_2, \\dots, x_n]$ we compute non-negative potentials $\\phi(\\boldsymbol{x}, i, a, b)$ that is a score of having a label $b$ at position $i$ if there is a label $a$ at position $i-1$. The potential can condition on the whole input sequence $\\boldsymbol{x}$.\n",
        "We will simplify notation in the rest of the notebook by not referring to it explicitly.\n",
        "The label of the first element of the sequence of course does not have a preceding label but we will assume there is a fixed label $0$ at position $0$ that precedes the first element $x_1$. Now we can define a potential of a sequence of labels $\\boldsymbol{y}=[y_1, y_2, \\dots, y_n]$ with a product of individual potentials:\n",
        "\n",
        "$$\n",
        "\\phi(\\boldsymbol{y}) = \\prod_{i=1}^{n} \\phi(i, y_{i-1}, y_i)\n",
        "$$\n",
        "\n",
        "This potential represents the unnormalized probability of that sequence. To normalize it we divide it with a sum of potentials of all possible sequences of labels for that same length.\n",
        "\n",
        "$$\n",
        "P(\\boldsymbol{y}) = \\frac{ \\phi(\\boldsymbol{y}) }{\\sum_{y' \\in Y} \\phi(\\boldsymbol{y'})}\n",
        "$$\n",
        "\n",
        "Computing this normalization requires dynamic programming algorithm called forward algorithm. For the details of this algorithm see the [references section](#scrollTo=References). SynJax provides multiple versions of this algorithm that have different pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPClKQaK1lr2"
      },
      "source": [
        "## Defining the distribution\n",
        "\n",
        "First we import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "k2xBXgj1-ZtP"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import synjax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpgUS9pX2adM"
      },
      "source": [
        "As usual with most probability distributions we need to work in the log-space in order to avoid issues with numerical stability. Therefore instead of *potentials* we define *log-potentials* that can take any real value, but ideally they should not go outside of domain $(-1e^5, 1e^5)$ to be safe from numerical errors with most floating data types.\n",
        "\n",
        "The shape of these log-potentials is $(n, m, m)$ where $n$ is the size of input sequcne and $m$ is the number of labels. Log-potential at position $[i, j, k]$ signifies the log-potential of position $i$ having a label $k$ given that the preceding position had label $j$.\n",
        "\n",
        "As mentioned earlier we assume that the label that precedes any input is by convention $0$. That means that log-potentials provided at $[0, 1\\!:, :]$ are ignored.\n",
        "\n",
        "An additional argument can be provided that specifies the length of a sequence. That is useful in case we want to process a batch of sequences of different lenght. The log-potentials tensor will have to be of the same shape for all sequences in the batch, but the provided lenght will inform SynJax how to do the padding correctly.\n",
        "\n",
        "Both log-potentials and lengths parameters can have preceding batch dimensions . Here is a simple Linear Chain CRF that is randomly initialized and has each sequence of a different length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "pNQLDn6vBB9_"
      },
      "outputs": [],
      "source": [
        "b, n, m = 3, 15, 5\n",
        "\n",
        "potentials = jax.random.uniform(jax.random.PRNGKey(0), (b, n, m, m))\n",
        "log_potentials = synjax.special.safe_log(potentials)\n",
        "lengths = jnp.array([5, 10, 15])\n",
        "dist = synjax.LinearChainCRF(log_potentials, lengths=lengths)\n",
        "\n",
        "dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl9f9AKODZ35"
      },
      "source": [
        "## Computing most likely structures and other interesting quantities\n",
        "\n",
        "We can compute many useful quantities with this object.\n",
        "\n",
        "Some return (batched) scalars:\n",
        "* `dist.log_prob(event)` finds log-probability of a particular sequence of transitions,\n",
        "* `dist.unnormalized_log_prob(event)` finds log-potential  of a particular sequence of transitions,\n",
        "* `dist.log_partition()` will return the sum of log of the normalization constant,\n",
        "* `dist.entropy()` would compute the entropy $H(\\operatorname{dist})$,\n",
        "* `dist.cross_entropy(dist2)` computes cross-entropy against some other distribution dist2 $H(\\operatorname{dist}, \\operatorname{dist2})$,\n",
        "* `dist.kl_divergence(dist2)` similarly computes $D_{\\operatorname{KL}}(\\operatorname{dist}||\\operatorname{dist2})$.\n",
        "\n",
        "Some returns structured objects:\n",
        "* `dist.argmax()` will return the most probable labeling,\n",
        "* `dist.top_k(k)` will return top k most probale labelings,\n",
        "* `dist.sample(key)` will return a sample of labeling for a given sampling key,\n",
        "* `dist.marginals()` will return marginal probability of each edge,\n",
        "* `dist.log_marginals()` will return log of marginal probabilities of each edge,\n",
        "* `dist.log_count()` will return log of the number of valid structures in the support.\n",
        "\n",
        "These structured objects are of ***the same shape*** as log-potentials. In the case of `argmax`, `top_k` and `sample` these are one-hot versions of log-potentials that mark each edge present in the output structure as 1 and non-present edges with 0. The shape of this tensor is $(n, m, m)$. If instead of edges we want labels that can be retrieved with one line `jnp.sum(event, axis=-2)`. Here are some examples of this. Notice how SynJax correctly pads each structure depending on its provided length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "uLNhyzi3-4LF"
      },
      "outputs": [],
      "source": [
        "event_of_edges = dist.argmax()  # has shape (b, n, m, m)\n",
        "event_of_labels = jnp.sum(event_of_edges, axis=-2)  # has shape (b, n, m)\n",
        "\n",
        "for i in range(dist.batch_shape[0]):\n",
        "  plt.title(f\"Best labeling from batch entry {i} with length {dist.lengths[i]}\")\n",
        "  plt.imshow(event_of_labels[i].T)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGbC57UEJkct"
      },
      "source": [
        "Another useful quantity is a marginal probability of each edge appearing in the correct labeling. Since visualizing marginals of edges is more difficult, we will marginalize marginals of labels in the same way: by summing over all the edges that end up in with same target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "v8qo7s9HKYbO"
      },
      "outputs": [],
      "source": [
        "marginals_of_edges = dist.marginals()  # has shape (b, n, m, m)\n",
        "marginals_of_labels = jnp.sum(marginals_of_edges, axis=-2)  # shape (b, n, m)\n",
        "\n",
        "for i in range(dist.batch_shape[0]):\n",
        "  plt.title(f\"Marginal probability from batch entry {i} \"\n",
        "            f\"with length {dist.lengths[i]}\")\n",
        "  plt.imshow(marginals_of_labels[i].T)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_YTE3nqK2_E"
      },
      "source": [
        "## Available algorithms\n",
        "\n",
        "All the quantities are computed with the same forward algorithm, however that algorithm can be implemented in two ways. The standard one is (mostly) sequential and processes a sequence from left-to-right in $O(m^2 n)$ time. There is an alternative implementation proposed by Hassan et al (2021) that processes the whole sequence in parallel with parallel runtime complexity of $O(m^3 \\log n)$. Rush (2020) reports that parallel implementation was faster for long sequences. In our benchmarks sequential implementation was always faster and took less memory. For that reason we have set sequential implementation as the default one, but user can override that by providing keyword argument `forward_algorithm=\"parallel\"` to any of the methods.\n",
        "\n",
        "If there are many structures that share a tie for the most probable structure calling `dist.argmax()` will not return one-hot tensor but instead have fractional counts. This is a very unlikely situation in most applications but if you want to be sure that it doesn't happen you can provide additional keyword argument `strict_max=True` to argmax which would incur a small runtime penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkXcAst21Dnf"
      },
      "source": [
        "## References\n",
        "\n",
        "* [Michael Collins -- Log-Linear Models, MEMMs, and CRFs](http://www.cs.columbia.edu/~mcollins/crf.pdf)\n",
        "* [Sutton and McCallum 2012 -- An Introduction to Conditional Random Fields](https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf)\n",
        "* [Lafferty et al 2001 -- Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Dataand Labeling Sequence Data](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162\u0026context=cis_papers)\n",
        "* [Hassan et al 2021 -- Temporal Parallelization of Inference in Hidden Markov Models](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512397)\n",
        "* [Rush 2020 -- TorchStruct: Deep Structured Prediction Library](https://aclanthology.org/2020.acl-demos.38.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "A7dZ6O0WBFMl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1BMDDp4LGd2WYy9myA4HiQjaUutWdHIF9",
          "timestamp": 1681663502392
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
