{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGi4Sii9AeR2"
      },
      "source": [
        "# Install\n",
        "Installing SynJax is simple -- one `pip install` will do the work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "ozzWR2JKdGsA"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU git+https://github.com/deepmind/synjax.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOXFQ3vDvJ3v"
      },
      "source": [
        "# Imports and Visualizations\n",
        "\n",
        "We will import just JAX, matplotlib and SynJax.\n",
        "Matplotlib will be used only through the simple `show` function that gives a heat map of a matrix. This function will be handy in visualizing representations of different structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "ozzWR5JKdGsA"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import synjax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "h7RX_lTUgGBC"
      },
      "outputs": [],
      "source": [
        "def show(x, title):\n",
        "  plt.title(title)\n",
        "  plt.imshow(jnp.asarray(x))\n",
        "  plt.show()\n",
        "\n",
        "key = jax.random.PRNGKey(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETYJGC2qiauo"
      },
      "source": [
        "# Linear Chain CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzMBLBw_Qp_R"
      },
      "source": [
        "One common structure prediction problem is a task of labeling elements of a sequence. For instance, we may want to label a sequence of words with their part of speach tags (verb, noun, adjective etc.).\n",
        "\n",
        "One simple way of accomplishing that task is by having a contextual embedding of each word in the sentence and then independently labeling each word with a classifier. However, that does not account for correlations between a sequence of tags. For example, this simple model doesn't capture the fact that in English a sequence of labels \"adjective noun\" is much more likely than \"noun adjective\". To solve that we can use a type of model called Linear Chain Conditional Random Field (CRF) that explicitly models this relation amoung labels.\n",
        "\n",
        "CRF assigns a non-negative score, called potential, to each pair of labels at each point in the sequence. Concretelly, for an input sequence $\\boldsymbol{x} = [x_1, x_2, \\dots, x_n]$ we compute non-negative potentials $\\phi(\\boldsymbol{x}, i, a, b)$ that is a score of having a label $b$ at position $i$ if there is a label $a$ at position $i-1$. The potential can condition on the whole input sequence $\\boldsymbol{x}$.\n",
        "We will simplify notation in the rest of the notebook by not referring to it explicitly.\n",
        "The label of the first element of the sequence of course does not have a preceding label but we will assume there is a fixed label $0$ at position $0$ that precedes the first element $x_1$. Now we can define a potential of a sequence of labels $\\boldsymbol{y}=[y_1, y_2, \\dots, y_n]$ with a product of individual potentials:\n",
        "\n",
        "$$\n",
        "\\phi(\\boldsymbol{y}) = \\prod_{i=1}^{n} \\phi(i, y_{i-1}, y_i)\n",
        "$$\n",
        "\n",
        "This potential represents the unnormalized probability of that sequence. To normalize it we divide it with a sum of potentials of all possible sequences of labels for that same length.\n",
        "\n",
        "$$\n",
        "P(\\boldsymbol{y}) = \\frac{ \\phi(\\boldsymbol{y}) }{\\sum_{y' \\in Y} \\phi(\\boldsymbol{y'})}\n",
        "$$\n",
        "\n",
        "Computing this normalization requires dynamic programming algorithm called forward algorithm that SynJax implements in an optimized vectorized way that can be compiled with XLA.\n",
        "\n",
        "## Defining the distribution\n",
        "\n",
        "As usual with most probability distributions we need to work in the log-space in order to avoid issues with numerical stability. Therefore instead of *potentials* we define *log-potentials* that can take any real value, but ideally they should not go outside of domain $(-1e^5, 1e^5)$ to be safe from numerical errors with most floating data types.\n",
        "\n",
        "The shape of these log-potentials is $(n, m, m)$ where $n$ is the size of input sequcne and $m$ is the number of labels. Log-potential at position $[i, j, k]$ signifies the log-potential of position $i$ having a label $k$ given that the preceding position had label $j$.\n",
        "\n",
        "As mentioned earlier we assume that the label that precedes any input is by convention $0$. That means that log-potentials provided at $[0, 1\\!:, :]$ are ignored.\n",
        "\n",
        "An additional argument can be provided that specifies the length of a sequence. That is useful in case we want to process a batch of sequences of different length. The log-potentials tensor will have to be of the same shape for all sequences in the batch, but the provided length will inform SynJax how to do the padding correctly.\n",
        "\n",
        "Both log-potentials and lengths parameters can have preceding batch dimensions . Here is a simple Linear Chain CRF that is randomly initialized and has each sequence of a different length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGOEAllzxDcl"
      },
      "outputs": [],
      "source": [
        "b, n, states = 3, 100, 50\n",
        "\n",
        "potentials = jax.random.uniform(jax.random.PRNGKey(0), (b, n, states, states))\n",
        "log_potentials = synjax.special.safe_log(potentials)\n",
        "lengths = jnp.array([n//3, n//2, n])\n",
        "dist = synjax.LinearChainCRF(log_potentials, lengths=lengths)\n",
        "\n",
        "dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5KDZ213xUd2"
      },
      "source": [
        "## Computing most likely structures and other interesting quantities\n",
        "\n",
        "We can compute many useful quantities with this object.\n",
        "\n",
        "Some return (batched) scalars:\n",
        "* `dist.log_prob(event)` finds log-probability of a particular sequence of transitions,\n",
        "* `dist.unnormalized_log_prob(event)` finds log-potential  of a particular sequence of transitions,\n",
        "* `dist.log_partition()` will return the sum of log of the normalization constant,\n",
        "* `dist.entropy()` would compute the entropy $H(\\operatorname{dist})$,\n",
        "* `dist.cross_entropy(dist2)` computes cross-entropy against some other distribution dist2 $H(\\operatorname{dist}, \\operatorname{dist2})$,\n",
        "* `dist.kl_divergence(dist2)` similarly computes $D_{\\operatorname{KL}}(\\operatorname{dist}||\\operatorname{dist2})$.\n",
        "\n",
        "Some returns structured objects:\n",
        "* `dist.argmax()` will return the most probable labeling,\n",
        "* `dist.top_k(k)` will return top k most probale labelings,\n",
        "* `dist.sample(key)` will return a sample of labeling for a given sampling key,\n",
        "* `dist.marginals()` will return marginal probability of each edge,\n",
        "* `dist.log_marginals()` will return log of marginal probabilities of each edge,\n",
        "* `dist.log_count()` will return log of the number of valid structures in the support.\n",
        "\n",
        "These structured objects are of ***the same shape*** as log-potentials. In the case of `argmax`, `top_k` and `sample` these are one-hot versions of log-potentials that mark each edge present in the output structure as 1 and non-present edges with 0. The shape of this tensor is $(n, m, m)$. If instead of edges we want labels that can be retrieved with one line `jnp.sum(event, axis=-2)`. Here are some examples of this. Notice how SynJax correctly pads each structure depending on its provided length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETrm4lLsxcCJ"
      },
      "outputs": [],
      "source": [
        "event_of_edges = dist.argmax()  # has shape (b, n, states, states)\n",
        "event_of_labels = jnp.sum(event_of_edges, axis=-2)  # has shape (b, n, states)\n",
        "\n",
        "for i in range(dist.batch_shape[0]):\n",
        "  plt.title(f\"Best labeling from batch entry {i} with length {dist.lengths[i]}\")\n",
        "  plt.imshow(event_of_labels[i].T)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo5Orwwzxu-J"
      },
      "source": [
        "Another useful quantity is a marginal probability of each edge appearing in the correct labeling. Since visualizing marginals of edges is more difficult, we will marginalize marginals of labels in the same way: by summing over all the edges that end up in with same target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWJPO9PCxyZV"
      },
      "outputs": [],
      "source": [
        "marginals_of_edges = dist.marginals()  # has shape (b, n, states, states)\n",
        "marginals_of_labels = jnp.sum(marginals_of_edges, axis=-2)  # shape (b, n, states)\n",
        "\n",
        "for i in range(dist.batch_shape[0]):\n",
        "  plt.title(f\"Marginal probability from batch entry {i} \"\n",
        "            f\"with length {dist.lengths[i]}\")\n",
        "  plt.imshow(marginals_of_labels[i].T)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8tMVyLKOp7u"
      },
      "source": [
        "# HMM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_pVPmyRx5MS"
      },
      "source": [
        "Hidden Markov Model (HMM) is a generative locally normalized model for sequential tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPShRuIyOrkD"
      },
      "outputs": [],
      "source": [
        "n, states, voc = 100, 50, 30\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 4)\n",
        "dist = synjax.HMM(init_logits=jax.random.normal(keys[0], (states,)),\n",
        "                  transition_logits=jax.random.normal(keys[1], (states,states)),\n",
        "                  emission_dist=jax.random.normal(keys[2], (states, voc)),\n",
        "                  observations=jax.random.randint(keys[3], (n,), 0, voc))\n",
        "\n",
        "def show_chain(chain, title):\n",
        "  show(chain.sum(-2).T, title)\n",
        "\n",
        "show_chain(dist.marginals(), \"marginals\")\n",
        "show_chain(dist.argmax(), \"argmax\")\n",
        "show_chain(dist.sample(key, 2).sum((0)), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_G1Vr40irWi"
      },
      "source": [
        "# Alignment CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvscSUMkN3u6"
      },
      "source": [
        "AlignmentCRF can model both monotone and non-monotone alignment. Log-potentials are a rectangular table of weights for matching two items. Monotone alignments support all probabilistic inferences, while non-monotone supports only argmax because other quantities are intractable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abQonSyAhep0"
      },
      "outputs": [],
      "source": [
        "n, m = 100, 150\n",
        "key = jax.random.PRNGKey(0)\n",
        "show(synjax.AlignmentCRF(jax.random.normal(key, (n, n)), alignment_type='non_monotone_one_to_one').argmax(), \"non_monotone_one_to_one\")\n",
        "show(synjax.AlignmentCRF(jax.random.normal(key, (n, n)), alignment_type='monotone_many_to_many').argmax(), \"monotone_many_to_many\")\n",
        "show(synjax.AlignmentCRF(jax.random.normal(key, (n, m)), alignment_type='monotone_one_to_many').argmax(), \"monotone_one_to_many\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FSJYUHtiMPl"
      },
      "source": [
        "Here are some additional quantities that can be computed for monotone alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "nj09DDVMdenY"
      },
      "outputs": [],
      "source": [
        "dist = synjax.AlignmentCRF(jax.random.normal(key, (n, m)),\n",
        "                           alignment_type='monotone_many_to_many')\n",
        "print(\"Entropy\", dist.entropy())\n",
        "print(\"Log-partition\", dist.log_partition())\n",
        "print(\"Log-count\", dist.log_count())\n",
        "\n",
        "show(dist.marginals(), \"marginals\")\n",
        "show(dist.sample(key, 2).sum(0), \"2 samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HepL5RV1B4_t"
      },
      "source": [
        "# CTC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T5XhX81D9qk"
      },
      "source": [
        "[Connectionist Temporal Classification (CTC)](https://distill.pub/2017/ctc/) is often used in speech recognition where the alignment between the input (speech signal) and gold output (words) is not observed in the training data. The assumption of this model is that the alignment is monotone (no reorderings), which is clearly the case in speech recognition. CTC is a wrapper on top of Alignment CRF from above. SynJax implementation of CTC provides not only the computation of the loss but all other useful quantities like argmax (forced alignment), sampling of alignments etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBwVr77EA7Ji"
      },
      "outputs": [],
      "source": [
        "gold_label_n, prediction_n, voc = 16, 32, 10_000\n",
        "logits = jax.random.normal(key, (prediction_n, voc))\n",
        "gold_labels = jax.random.randint(key, (gold_label_n,), 0, voc)\n",
        "dist = synjax.CTC(log_potentials=logits, labels=gold_labels)\n",
        "show(dist.marginals(), \"marginals\")\n",
        "show(dist.argmax(), \"forced alignment\")\n",
        "show(dist.sample(key), \"sampled alignment\")\n",
        "print(\"CTC loss\", dist.loss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwJD5Xr9in7H"
      },
      "source": [
        "# Spanning Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mhl1KA0_yhG"
      },
      "source": [
        "All types of spanning trees are accessed trough `synjax.SpanningTreeCRF` class. Naturally, it takes (optionally batched) matrix of log-potentials, lengths per instance and flags that signify if the spanning tree is directed (it is an arborescence), if it is projective (used mostly in NLP) and if it it has only one root outgoing edge. Below are some examples of how different variations of spanning tree distribution could be instantiated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZPkPqiki33"
      },
      "source": [
        "## Directed Non-Projective Dependency CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "3IdFdQ3LklrM"
      },
      "outputs": [],
      "source": [
        "n = 20\n",
        "\n",
        "log_potentials = jax.random.normal(key, (n, n))\n",
        "dist = synjax.SpanningTreeCRF(log_potentials, directed=True,\n",
        "                              projective=False, single_root_edge=True)\n",
        "\n",
        "show(dist.marginals(), \"marginals\")\n",
        "# Line below may be slightly slower on first run because of Numba compilation.\n",
        "show(dist.argmax(), \"argmax\")\n",
        "show(dist.sample(key, 2).sum((0)), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1pyp53HvDcG"
      },
      "source": [
        "## Undirected Non-Projective Dependency CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "yu5FmWb4u0IU"
      },
      "outputs": [],
      "source": [
        "n = 20\n",
        "\n",
        "log_potentials = jax.random.normal(key, (n, n))\n",
        "dist = synjax.SpanningTreeCRF(log_potentials, directed=False,\n",
        "                              projective=False, single_root_edge=True)\n",
        "\n",
        "show(dist.marginals(), \"marginals\")\n",
        "show(dist.argmax(), \"argmax\")\n",
        "show(dist.sample(key, 3).sum((0)), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZgQ7kz4izom"
      },
      "source": [
        "## Directed Projective Dependencies CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "kOvo8XiehTZD"
      },
      "outputs": [],
      "source": [
        "n = 20\n",
        "\n",
        "log_potentials = jax.random.normal(jax.random.PRNGKey(0), (n, n))\n",
        "dist = synjax.SpanningTreeCRF(log_potentials, directed=True,\n",
        "                              projective=True, single_root_edge=True)\n",
        "\n",
        "show(dist.marginals(), \"marginals\")\n",
        "show(dist.argmax(), \"argmax\")\n",
        "show(dist.sample(key, 3).sum((0)), \"samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i2oTqvRjMt8"
      },
      "source": [
        "# Constituency Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Driz7WVyimbK"
      },
      "source": [
        "## Tree CRF\n",
        "\n",
        "  The model structure is very similar to [Stern et al (2017)](https://aclanthology.org/P17-1076.pdf) except SynJax\n",
        "  additionally supports properly normalizing the distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "ohjb1SyJd1me"
      },
      "outputs": [],
      "source": [
        "nt = 1\n",
        "n = 15\n",
        "\n",
        "log_potentials = jax.random.normal(key, (n, n, nt))\n",
        "dist = synjax.TreeCRF(log_potentials, lengths=None)\n",
        "\n",
        "show(dist.marginals().sum(-1), \"marginals\")\n",
        "show(dist.argmax().sum(-1), \"argmax\")\n",
        "show(dist.sample(key, 2).sum((0, -1)), \"samples\")\n",
        "show(dist.top_k(2)[0].sum((0, -1)), \"top_k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfxjHpNhjQdR"
      },
      "source": [
        "## PCFG\n",
        "\n",
        "  \n",
        "Note that this is a conditional PCFG, i.e. it is a distribution over trees\n",
        "provided by PCFG conditioned by a provided sentence. Because of that calling\n",
        "`dist.log_probability(tree)` returns a `p(tree | sentence; pcfg)`. To get a\n",
        "joint probability of a tree and a sentence `p(tree, sentence ; pcfg)` call\n",
        "`dist.unnormalized_log_probability(tree)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "2g3a74wTjRDd"
      },
      "outputs": [],
      "source": [
        "t, pt, n, voc = 4, 8, 10, 10\n",
        "\n",
        "normal = jax.random.normal\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 4)\n",
        "\n",
        "dist = synjax.PCFG(\n",
        "    root=normal(keys[0], (nt,)),\n",
        "    rule=normal(keys[1], (nt, nt+pt, nt+pt)),\n",
        "    emission=normal(keys[2], (pt, voc)),\n",
        "    word_ids = jax.random.randint(keys[3], (n,), 0, voc)\n",
        ")\n",
        "\n",
        "\n",
        "show(dist.marginals().chart.sum(-1), \"marginals\")\n",
        "show(dist.argmax().chart.sum(-1), \"argmax\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r72o11UjR6U"
      },
      "source": [
        "## Tensor-Decomposition PCFG\n",
        "\n",
        "[Cohen et al (2013)](https://aclanthology.org/N13-1052.pdf#page=8) showed that PCFG with large number of non-terminals can be\n",
        "approximated using CPD tensor decomposition. [Yang et al (2022)](https://aclanthology.org/2022.naacl-main.353.pdf) used this to\n",
        "do efficient grammar induction with large number of non-terminals and\n",
        "relatively small rank dimension. They avoid tensor-decomposition step by\n",
        "keeping all parameters always in the rank space and enforcing all decomposed\n",
        "rules to be normalized. Just like a regular PCFG implementation, the implementation of Tensor-Decomposition PCFG is also a conditional model for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "test": {
            "output": "ignore",
            "skip": false,
            "timeout": 300
          }
        },
        "id": "5yPVXVhmjLCn"
      },
      "outputs": [],
      "source": [
        "nt, pt, n, voc, rank = 4, 8, 10, 10, 6\n",
        "\n",
        "normal = jax.random.normal\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 5)\n",
        "\n",
        "dist = synjax.TensorDecompositionPCFG(\n",
        "    root=normal(keys[0], (nt,)),\n",
        "    nt_to_rank=normal(keys[1], (nt, rank)),\n",
        "    rank_to_left_nt=normal(keys[2], (rank, nt+pt)),\n",
        "    rank_to_right_nt=normal(keys[3], (rank, nt+pt)),\n",
        "    emission=normal(keys[4], (pt, voc)),\n",
        "    word_ids = jax.random.randint(keys[4], (n,), 0, voc)\n",
        ")\n",
        "\n",
        "show(dist.marginals().chart.sum(-1), \"marginals\")\n",
        "show(dist.mbr(marginalize_labels=True).chart.sum(-1), \"MBR\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
